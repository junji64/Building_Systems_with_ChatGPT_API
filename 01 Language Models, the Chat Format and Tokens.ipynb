{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "819b7b1a",
   "metadata": {},
   "source": [
    "### reference\n",
    "\n",
    "* https://learn.deeplearning.ai/courses/chatgpt-building-system/lesson/1/introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc75cb02",
   "metadata": {},
   "source": [
    "# Building Systems with the ChatGPT API\n",
    "\n",
    "**ChatGPT API**를 사용하여 **시스템을 구축**하는 과정에 오신 것을 환영합니다.\n",
    "\n",
    "**시스템을 구축**하는 과정에는 단일 프롬프트나 대규모 언어 모델에 대한 단일 호출보다 훨씬 더 많은 것이 필요합니다. 여기에서 **LLM을 사용**하여 **복잡한 애플리케이션을 구축**하기 위한 모범 **사례를 공유**하고자 합니다. 우리는 언어 모델에 대한 여러 호출을 연결하고 이전 호출의 출력에 따라 다른 명령을 사용하고 때로는 외부 소스에서 항목을 조회하는 **고객 지원 시스템을 구축**하는 실행 예제를 사용합니다.\n",
    "\n",
    "예를 들어 \"판매 중인 TV에 대해 알려주세요.\"와 같은 사용자 입력이 주어지면 다음 단계를 사용하여 이를 처리합니다.\n",
    "\n",
    "* 먼저 **입력 내용을 평가**하여 증오심 표현과 같은 문제가 있는 콘텐츠가 포함되어 있지 않은지 확인할 수 있습니다. \n",
    "* 다음으로 시스템은 **입력을 처리**합니다. 이것이 **어떤 유형의 쿼리인지 식별**합니다. 불만 또는 제품 정보 요청 등입니까? \n",
    "* 제품 문의라고 판단되면 TV에 대한 관련 정보를 검색하고 **언어 모델을 사용하여 유용한 답변을 작성**합니다.\n",
    "* 마지막으로 **출력을 확인**하여 부정확하거나 부적절한 답변과 같은 문제가 없는지 확인합니다. \n",
    "\n",
    "이 과정에서 확인할 수 있는 주제는 **응용 프로그램이 종종 최종 사용자에게 보이지 않는 여러 내부 단계를 필요로 한다는 것입니다**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf42ef8",
   "metadata": {},
   "source": [
    "#  Language Models, the Chat Format and Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0807e3b3",
   "metadata": {},
   "source": [
    "## Large Language Model\n",
    "\n",
    "##### Text generation process\n",
    "\n",
    "| I love eating |  -------------------------------------------------- |\n",
    "| ------------------|-------------------|\n",
    "| [prompt] | bagles with cream cheese |\n",
    "|        | my mothers's meatloaf |\n",
    "|        | out with friends |\n",
    "\n",
    "##### Supervised Learning (x --> y)\n",
    "\n",
    "레스토랑 리뷰어 감정 분류\n",
    "\n",
    "| Input x |  Output y |\n",
    "| ------------------|-------------------|\n",
    "| The pastromi sandwich was great | positive |\n",
    "| Service was slow and the food was so~so | netative |\n",
    "| The earl grey tea was fantastic. | positive |\n",
    "| [Best pizza I've aver had!] | [positive] |\n",
    "\n",
    "\n",
    "<img src=\"supervised.png\" width=\"400\">\n",
    "\n",
    "##### Large Language Model (LLM) : How it works\n",
    "\n",
    "지도 학습(x->y)을 사용하여 다음 단어를 반복적으로 예측하는 방식으로 언어 모델을 구축합니다.\n",
    "\n",
    "My favorite food is a bagel with cream cheese and lox.\n",
    "\n",
    "| Input x |  Output y |\n",
    "| ------------------|-------------------|\n",
    "| My favorite food is a | bagel |\n",
    "| My favorite food is a bagel | with |\n",
    "| My favorite food is a bagel with | cream |\n",
    "| My favorite food is a bagel with cream | cheese |\n",
    "\n",
    "\n",
    "##### Two types of large language models (LLMs)\n",
    "\n",
    "**Base LLM**\n",
    "\n",
    "텍스트 학습 데이터를 기반으로 다음 단어를 예측합니다.\n",
    "\n",
    "*[Once up a time, there was a unicorn]*\n",
    "<br>\n",
    "that lived in a magical forest with all her unicorn friends.\n",
    "<br>\n",
    "<br>\n",
    "*[What is the capital of France?]*\n",
    "<br>\n",
    "What is France's largest city?\n",
    "<br>\n",
    "What is France's population?\n",
    "<br>\n",
    "What is the currency of France?\n",
    "\n",
    "\n",
    "**Instruction Tuned LLM**\n",
    "\n",
    "지시를 따르려고 노력합니다\n",
    "\n",
    "*[What is the capital of France?]*\n",
    "<br>\n",
    "The capital of France is Paries.\n",
    "\n",
    "**Getting from a Base LLM to an Instruction Tuned LLM**\n",
    "1. 많은 데이터에 대해 Base LLm을 훈련합니다.\n",
    "2. 모델을 추가로 훈련합니다.\n",
    "   - 입력 명령에 따라 출력을 하는 예들로 **미세 조정**합니다.\n",
    "   - 도움이 되고(helpful), 정직하고(honest), 무해한(harmless)지 (HHH) 여부와 같은 기준에 따라 **다양한 LLM 결과물의 품질에 대한 인간 평가**를 얻습니다.\n",
    "   - **LLM을 조정**하여 더 높은 등급의 출력을 생성할 확률을 높입니다(using RLHF: Reinforcement Learning from Human Feedback)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e79ede",
   "metadata": {},
   "source": [
    "## Setup\n",
    "#### Load the API key and relevant Python libaries.\n",
    "이 과정에서는 OpenAI API 키를 로드하는 몇 가지 코드를 제공했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a32cad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c9c3fd",
   "metadata": {},
   "source": [
    "#### helper function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9adeff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # 이것는 모델 출력의 무작위성 정도를 말합니다.[0~1]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ca1fb8",
   "metadata": {},
   "source": [
    "## Prompt the model and get a completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6be32d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(\"What is the capital of France?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da569f56",
   "metadata": {},
   "source": [
    "## Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d1352c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pilpolol\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(\"Take the letters in lollipop and reverse them\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7aca499d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-o-p-i-l-l-o-l\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(\"\"\"Take the letters in l-o-l-l-i-p-o-p and reverse them\"\"\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ac6d44",
   "metadata": {},
   "source": [
    "<img src=\"token.PNG\" width=400>\n",
    "\n",
    "영어 입력의 경우 1 token 은 약 4 글자, 즉 단어의 3/4 정도입니다.\n",
    "\n",
    "token 한도\n",
    "* 모델마다 입력 \"컨텍스트\" + 출력 완료의 토큰 수에 대한 제한이 다릅니다.\n",
    "* gpt3.5-turbo ~ 4000 토큰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e0d54f",
   "metadata": {},
   "source": [
    "### System, User and Assistant Messages\n",
    "```\n",
    "messages =\n",
    "[\n",
    " {\"role\": \"system\",\n",
    "  \"content\": \"You are an assistant ...\"},\n",
    " {\"role\": \"user\",\n",
    "  \"content\": \"Tell me a joke \"},\n",
    " {\"role\": \"assistant\",\n",
    "  \"content\": \"Why did the chicken ...\"},\n",
    "  ...\n",
    "```\n",
    "<img src=\"mesg.PNG\" width=350>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967e6f60",
   "metadata": {},
   "source": [
    "## Helper function (chat format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9962046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_from_messages(messages, \n",
    "                                 model=\"gpt-3.5-turbo\", \n",
    "                                 temperature=0, \n",
    "                                 max_tokens=500):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, # this is the degree of randomness of the model's output\n",
    "        max_tokens=max_tokens, # the maximum number of tokens the model can ouptut \n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40b29c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh the happy carrot, so bright and so orange,\n",
      "Dancing in the garden, free to explore and forage.\n",
      "With a leafy green top and a cheerful grin,\n",
      "This little veggie is sure to win. \n",
      "Beneath the sun, it grows and thrives,\n",
      "A happy carrot, so full of life!\n"
     ]
    }
   ],
   "source": [
    "messages =  [  \n",
    "{'role':'system', \n",
    " 'content':\"You are an assistant who responds \\\n",
    "            in the style of Dr Seuss.\"},    \n",
    "{'role':'user', \n",
    " 'content':\"write me a very short poem about a happy carrot\"},  \n",
    "] \n",
    "response = get_completion_from_messages(messages, temperature=1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a796a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once there was a cheerful carrot named Charlie who always brought smiles to everyone in the garden.\n"
     ]
    }
   ],
   "source": [
    "# length\n",
    "messages =  [  \n",
    "{'role':'system',\n",
    " 'content':'All your responses must be one sentence long.'},    \n",
    "{'role':'user',\n",
    " 'content':'write me a story about a happy carrot'},  \n",
    "] \n",
    "response = get_completion_from_messages(messages, temperature =1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f28b223e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a garden so bright, there lived a happy carrot, orange and upright.\n"
     ]
    }
   ],
   "source": [
    "# combined\n",
    "messages =  [  \n",
    "{'role':'system',\n",
    " 'content':\"You are an assistant who responds in the style of Dr Seuss. \\\n",
    "All your responses must be one sentence long.\"},    \n",
    "{'role':'user',\n",
    " 'content':\"write me a story about a happy carrot\"},\n",
    "] \n",
    "response = get_completion_from_messages(messages, \n",
    "                                        temperature =1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8eef8254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_and_token_count(messages, \n",
    "                                   model=\"gpt-3.5-turbo\", \n",
    "                                   temperature=0, \n",
    "                                   max_tokens=500):\n",
    "    \n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, \n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    \n",
    "    content = response.choices[0].message.content\n",
    "    \n",
    "    token_dict = {\n",
    "        'prompt_tokens':response.usage.prompt_tokens,\n",
    "        'completion_tokens':response.usage.completion_tokens,\n",
    "        'total_tokens':response.usage.total_tokens,\n",
    "    }\n",
    "    return content, token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b91878c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "{'role':'system', \n",
    " 'content':\"You are an assistant who responds in the style of Dr Seuss.\"},    \n",
    "{'role':'user',\n",
    " 'content':\"write me a very short poem about a happy carrot\"},  \n",
    "] \n",
    "response, token_dict = get_completion_and_token_count(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51d13fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, the happy carrot, so bright and so bold,\n",
      "In the garden, its story is joyfully told.\n",
      "With a leafy green top and a vibrant orange hue,\n",
      "It dances and sings, bringing smiles to you.\n",
      "\n",
      "It grows in the earth, soaking up the sun,\n",
      "Laughing and playing, oh what fun!\n",
      "So here's to the carrot, so cheerful and merry,\n",
      "Bringing happiness to all, let's give a big hurray!\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bcf28818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt_tokens': 35, 'completion_tokens': 91, 'total_tokens': 126}\n"
     ]
    }
   ],
   "source": [
    "print(token_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
